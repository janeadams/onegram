<!DOCTYPE html>
<html>

<head>
    <meta charset="utf=8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="styles/main.css">
    <link rel="stylesheet" type="text/css" href="styles/query.css">
    <link rel="stylesheet" type="text/css" href="styles/chart.css">
    <link href="https://fonts.googleapis.com/css?family=Pangolin&display=swap" rel="stylesheet">
</head>

<body>
<header>
    <a href="https://storywrangling.org">
        <div id="logo">
        <img src="img/storywrangler_small.svg" alt="Story Wrangler: stories from the Twitterverse. From the University of Vermont Complex Systems Center">
        <p>from the University of Vermont Computational Story Lab</p>
        </div>
    </a>
    <div id="navigation">
        <ul id="menu">
            <!--<li><a href="about.html">About</a></li>-->
        </ul>
        <ul class="social">
            <li><a href="https://twitter.com/compstorylab"><img src="img/social_twitter.svg" alt="Follow the Computational Story Lab on Twitter!"></a></li>
            <li><a href="https://www.instagram.com/computationalstorylab/"><img src="img/social_insta.svg" alt="Follow the Computational Story Lab on Instagram!"></a></li>
        </ul>
    </div>
</header>
<main>
    <section class="textblock">
    <h1>What&rsquo;s the Story? </h1>

    <p>Language is humanity&rsquo;s greatest social technology. While we&rsquo;ve been communicating for thousands of years, it is only recently that we&rsquo;ve begun sharing content online&mdash;posting selfies, searching for validation, and expressing our uninformed opinions in hot takes and quote tweets. Making sense of all that is being said is a tall order, best suited to a suite of algorithms. Computers can digest bits of language and help us describe, explain, and understand cultural phenomenon at the scale of human populations. The StoryWrangler instrument reflects our first step towards wrestling the day&rsquo;s events into coherence. It is an approximate daily leaderboard for language popularity around the globe.</p>

    <h2>The Data</h2>

    <p>While most of our online activity is not broadcast publicly, the things we do choose to share are important, and worthy of careful enumeration. One prominent source for real-time global information is social media. Half of a billion messages are posted to Twitter every day! Written on post-it notes, they would wrap around the Earth&rsquo;s equator in a neon hug full of politics, pop music, and sports.</p>

    <p>At the Computational Story Lab, since 2008 we have collected a random 10% of all public messages using Twitter&rsquo;s Decahose API.  Overall, our collection comprises roughly 150 billion messages requiring 100TB of storage. Inspired by the Google n-grams project which smashes books into bits, we&rsquo;ve meticulously parsed these tweets into daily frequencies of words, 2-word phrases, and three word expressions.  Scientists call these different types &ldquo;n-grams&rdquo; for n = 1, 2, 3.</p>

    <p>We&rsquo;re able to categorize tweets into roughly 170 languages, but for now we&rsquo;re simply combining them all into a single bag when tallying the votes. To explore how people are feeling, at the site for our flagship instrument http://hedonometer.org we use this same data to visualize daily sentiment in 9 languages. Importantly, this site offers visitors the opportunity to toggle retweets on and off, allowing for the inclusion or exclusion of social amplification. While we&rsquo;re not able to share individual tweets, we do provide an API for exporting data at the daily resolution. Source code associated with the API can be found here, and the pattern match used to parse tweets can be found here.</p>

    <h2>The Visualization</h2>

    <p>Sorting frequencies by popularity, we find some n-grams that appear very consistently in time. For example, &ldquo;god&rdquo; is roughly the 300th most commonly used word each day.  Others rise and fall cyclicly. &ldquo;Friday&rdquo;, &ldquo;New Year&rsquo;s Eve&rdquo;, and &ldquo;Winter Olympics&rdquo; spike every week, year, and four years respectively. For one glorious day in 2015, :blush: was the most commonly used word! Many many many more phrases appear very rarely. </p>

    <p>The shapes of our collective attention are quite universal. Holiday mentions build exponentially to a crescendo before falling precipitously. Movies produce a more symmetric pattern centered on their release date. And mentions of individual years reliably plateau chronologically, with a volcanic rim bounded by their beginning and end.</p>

    <p>We hope journalists, linguists, political scientists, and the extremely online alike are able to use this data to characterize collective attention in new and interesting ways.  Have at it! And tell us if you have trouble. </p>

    <h2>Some of the many gory details</h2>

    <p><img id="pattern_match" src="img/pattern_match.png"> Parsing tweets into components is harder than we expected, so we have a few specifics to share about how it is being done. Regarding the definition of an n-gram, 1-grams are bounded by spaces. 2-grams comprise a pair of 1-grams with a space between them. Punctuation is hard so we&rsquo;ve left it in.</p>

    <p>Our pattern match is restricted to n-grams containing at least one latin character or emoji. Hashtags and handles are in. URLs and carriage returns and other unicode trouble are out. The database is case sensitive, so if you&rsquo;re looking for the popularity of playing card game &ldquo;Trump suits&rdquo;, you&rsquo;ll want to use the lower case version &ldquo;trump&rdquo;.</p>

    <p>For simplicity, and to avoid requiring atomic clocks w/relativitistic space-time corrections, we use Eastern time in the US when assigning n-grams to a calendar day. Ranks are thresholded at 1 million, so if a phrase is less popular than this cutoff on an individual day, we don&rsquo;t offer information on its prevalence. Provided the computers aren&rsquo;t on &#x1F525;&#x1F525;&#x1F525;, daily ranks will be updated within roughly 24-hours. Somewhat confusingly, more popular words have lower rank and rare words have large rank. Ranks are calculated within a single category of n-gram, so while you can plot &ldquo;NSync&rdquo; against &ldquo;Backstreet Boys&rdquo;, their relative rankings are computed against different collections.</p>

    <h2>Obligatory Warnings</h2>

    <p>As a reflection of global events, Twitter data is problematic for several reasons. &ldquo;Tweets are a non-representative subsample of utterances made by a non-representative subsample of Earth&rsquo;s population.&rdquo; Pew surveys suggest only 1 in 5 adults Americans use the service. This data reflects only a random 10% of messages. This means that for exceedingly rare words, while the frequencies reported here are roughly one tenth of the true values, the rankings are likely to be unreliable. While we make no attempt to remove automated account activity, selecting the &ldquo;exclude retweets&rdquo; option will likely reduce the contribution of coordinated bot behavior. </p>

    <h2>Attribution</h2>

    <p>If you use the data from our site in a scientific study, please cite it.</p>

    <h2>Related Publications</h2>

    <p>A few example studies we&rsquo;ve undertaken recently using this data:</p>

    <h4>Allotaxonometry and rank-turbulence divergence: A universal instrument for comparing complex systems.</h4>
    <p>P. S. Dodds, J. R. Minot, M. V. Arnold, T. Alshaabi, J. L. Adams, D. R. Dewhurst, T. J. Gray, M. R. Frank, A. J. Reagan, C. M. Danforth In Review. 2020. [pdf] [arXiv] [online appendix] [code] [thread]</p>

    <h4>How the world&rsquo;s collective attention is being paid to a pandemic: COVID-19 related 1-gram time series for 24 languages on Twitter.</h4>
    <p>T. Alshaabi, M. V. Arnold, J. R. Minot, J. L. Adams, D. R. Dewhurst, A. J. Reagan, R. Muhamad, C. M. Danforth, P. S. Dodds. In Review. 2020. [pdf] [arXiv] [online appendix] [animation] [thread] </p>

    <h4>The growing echo chamber of social media: Measuring temporal and social contagion dynamics for over 150 languages on Twitter for 2009-2020.</h4>
    <p>T. Alshaabi, D. R. Dewhurst, J. R. Minot, M. V. Arnold, J. L. Adams, C. M. Danforth, P. S. Dodds. In Review. 2020. [pdf] [arXiv] [online appendix] [thread]</p>

    <h4>Fame and Ultrafame: Measuring and comparing daily levels of &lsquo;being talked about&rsquo; for United States&rsquo; presidents, their rivals, God, countries, and K-pop.</h4>
    <p>P. S. Dodds, J. R. Minot, M. V. Arnold, T. Alshaabi, J. L. Adams, D. R. Dewhurst, A. J. Reagan, C. M. Danforth. In Review. 2020. [pdf] [arXiv] [thread] [online appendix]</p>

    <h4>The shocklet transform: A decomposition method for the identification of local, mechanism-driven dynamics in sociotechnical time series.</h4>
    <p>D. R. Dewhurst, T. Alshaabi, D. Kiley, M. V. Arnold, J. R. Minot, C. M. Danforth, P. S. Dodds.<br/>
        EPJ Data Science. 2020. [pdf] <a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-020-0220-x" target="_blank">[journal]</a> <a href="https://arxiv.org/abs/1906.11710" target="_blank">[arXiv]</a> <a href="http://compstorylab.org/shocklets/ranked_shock_weighted_interactive.html" target="_blank">[online appendix]</a> <a href="https://gitlab.com/compstorylab/discrete-shocklet-transform" target="_blank">[code]</a> <a href="https://twitter.com/compstorylab/status/1227946808321728512" target="_blank">[thread]</a></p>

    <h2>Contact</h2>

    <p>Share the interesting things you notice! Questions and comments can be sent to <a href="@mailto:jane.adams@uvm.edu">Jane.Adams@uvm.edu</a></p>

    <h2>Acknowledgements</h2>

    <p>The front end of the site was built by Jane Adams. The back end was built by Michael Arnold, Thayer Alshaabi, Josh Minot, and Andy Reagan. The project is lead by Peter Dodds and Chris Danforth, and their research group the Computational Story Lab at the University of Vermont. Several students have contributed including:</p>

    <p>Sharon Alajajian, Nicholas Allgaier, Catherine Bliss, Eric Clark, Emily Cody, Ethan Davis, Todd DeLuca, Suma Desu, David Dewhurst, Danne Elbers, Kameron Decker Harris, Fletcher Hazlehurst, Sophie Hodson, Kayla Horak, Ben Emery, Mike Foley, Morgan Frank, Ryan Gallagher, Darcy Glenn, Sandhya Gopchandani, Kelly Gothard, Tyler Gray, Max Green, Laura Jennings, Dilan Kiley, Isabel Kloumann, Ben Kotzen, Paul Lessard, Ross Lieb-Lappen, Kelsey Linnell, Ashley McKhann, Andy Metcalf, Tom McAndrew, Sven McCall, Henry Mitchell, Lewis Mitchell, Kate Morrow, Eitan Pechenick, Michael Pellon, Aaron Powers, Andy Reagan, John Ring IV, Abby Ross, Lindsay Ross, Aaron Schwartz, Anne-Marie Stupinski, Matt Tretin, Lindsay Van Leir, Colin Van Oort, Brendan Whitney, and Jake Williams.</p>

    <p>Funding for the project has been provided by the Vermont Complex Systems Center, MassMutual and the University of Vermont. Many thanks and acknowledgments go to these lovely people:</p>

    <p>Mike Austin, Jim Bagrow, Josh Bongard, Josh Brown, Jim Burgmeier, Melody Burkins, Kate Danforth, Andrea Elledge, Maggie Eppstein, Bill Gottesman, Laurent Hebert-Dufresne, John Kaehny, Jim Lawson, Juniper Lovato, Aimee Picchi, Andrew Reece, Tony Richardson, Taylor Ricketts, Melissa Rubinchuk, Brian Tivnan, John Tucker and Toph Tucker.</p>

    <p>And as always, thank you for your tweets. They are mostly good.</p>
    </section>
</main>
<footer>
    <section id="thanks">
        <p>A research tool from:</p>
        <a href="https://twitter.com/compstorylab"><img id="storylab" src="img/footer/storylab.svg" alt="Computational Story Lab"></a>
        <a href="http://vermontcomplexsystems.org/"><img id="csc" src="img/footer/csc_roboctopus.svg" alt="The Vermont Complex Systems Center"></a>
        <a href="http://vermontcomplexsystems.org/partner/MMCOE/"><img id="mm-coe" src="img/footer/mm-coe.svg" alt="Mass Mutual Center of Excellence in Complex Systems and Data Science"></a>
        <!--<a href="https://www.uvm.edu/cems"><img id="uvm-cems" src="img/footer/uvm-cems.svg" alt="University of Vermont College of Engineering and Mathematical Sciences"></a>-->
    </section>
</footer>
</body>

</html>